{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "179c4d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b702b97",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f8a8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', 'I', 'am', 'Hemant', '.', 'I', 'am', 'from', 'computer', 'department']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text=\"Hey, I am Hemant. I am from computer department\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acab18e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey, I am Hemant.', 'I am from computer department']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"Hey, I am Hemant. I am from computer department\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070b4db",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46fd4063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "words=[\"wait\",\"waiting\",\"waited\",\"waits\"]\n",
    "ps=PorterStemmer()\n",
    "for i in words:\n",
    "    rootword=ps.stem(i)\n",
    "    print(rootword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d34af6",
   "metadata": {},
   "source": [
    "### Limmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c92195",
   "metadata": {},
   "source": [
    "Limmatisation is better than stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9602324c",
   "metadata": {},
   "source": [
    "#### Stemming code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aaf8e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studi\n",
      "studi\n",
      "cri\n",
      "cri\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "text=\"studies studying cries crying\"\n",
    "tokenization=nltk.word_tokenize(text)\n",
    "for i in tokenization:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57ffcf",
   "metadata": {},
   "source": [
    "#### Lemmiatisation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb6bed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study\n",
      "studying\n",
      "cry\n",
      "cry\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl=WordNetLemmatizer()\n",
    "text=\"studies studying cries crying\"\n",
    "tokenization=nltk.word_tokenize(text)\n",
    "for i in tokenization:\n",
    "    print(wl.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7abc94",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfbcaca",
   "metadata": {},
   "source": [
    "stop words : the, is, but, are ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b0bbae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', 'I', 'Hemant', '.', 'I', 'computer', 'department']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data=\"Hey, I am Hemant. I am from computer department\"\n",
    "stWords=set(stopwords.words('english'))\n",
    "words = word_tokenize(data)\n",
    "wordFiltered=[]\n",
    "\n",
    "for i in words:\n",
    "    if i not in stWords:\n",
    "        wordFiltered.append(i)\n",
    "\n",
    "print(wordFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e705599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "{\"shouldn't\", 'we', 'here', 'most', 'have', 'after', 'do', 'while', 'himself', 'until', 'with', 'aren', \"needn't\", 'ma', 'y', 'that', 'their', 'then', 'up', 'where', 'very', 't', \"mightn't\", 'when', 'above', \"doesn't\", 'out', 'each', 'hasn', 'from', \"hasn't\", 'through', 'both', \"won't\", 'our', 'm', 'can', 'more', \"you've\", 're', \"you'll\", \"should've\", \"wasn't\", 'and', 'needn', 'mightn', 'has', 'how', 'yours', 'was', 'as', 'there', 'were', 'myself', 'off', 'those', 'not', 'been', \"that'll\", 'be', 'which', 'such', 'your', 'why', 'o', 'ourselves', \"couldn't\", 'couldn', 'me', \"hadn't\", 'a', 'wouldn', 'my', 'further', 'being', 'they', 'so', \"you're\", 'same', 'did', 'between', 'an', \"haven't\", 'haven', \"mustn't\", 'will', 'during', 'ain', 'her', 'below', 'only', \"she's\", 'hers', 'having', 'on', 'than', 'some', 'is', 'hadn', \"aren't\", 'doing', 'had', 'because', 'what', 'yourself', 'it', 'under', 'isn', 'them', 'its', 'shouldn', 'he', 'll', 'shan', 'this', 'for', 'other', 'by', 'into', 'now', 'few', 'themselves', 'wasn', 'the', 'just', 's', 'weren', 'won', 'if', 'again', 'should', 'does', \"shan't\", 'him', 'too', 'are', 'about', \"isn't\", 'doesn', 'his', 'at', 'you', 'd', 'to', 'of', 'don', 'over', 'theirs', 'down', 'against', \"wouldn't\", 'mustn', 'before', 'she', 'or', 'these', 'all', 'whom', 'once', 'itself', \"you'd\", 'own', 'who', 'i', 'but', 'any', 'yourselves', \"didn't\", \"don't\", 'no', \"it's\", 'in', 'nor', 'didn', 've', 'am', 'herself', 'ours', \"weren't\"}\n"
     ]
    }
   ],
   "source": [
    "print(len(stWords))\n",
    "print(stWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566544f",
   "metadata": {},
   "source": [
    "### POS Tagging (Part Of Speech Tagging) or POST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f45c4",
   "metadata": {},
   "source": [
    "It is a process of converting a sentence to forms â€“ list of words, list of tuples \n",
    "(where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, \n",
    "and signifies whether the word is a noun, adjective, verb, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05a03183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NN'), ('World', 'NN')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "tagging = DefaultTagger(\"NN\")\n",
    "tagging.tag([\"Hello\", \"World\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a4333fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Hello', 'NN'), ('World', 'NN')], [('hey', 'NN'), ('there', 'NN')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "tagging = DefaultTagger(\"NN\")\n",
    "tagging.tag_sents([[\"Hello\", \"World\"], [\"hey\", \"there\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a8608b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', 'hey', 'there']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "untag([('Hello', 'NN'), ('World', 'NN'), ('hey', 'NN'), ('there', 'NN')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2506567",
   "metadata": {},
   "source": [
    "### Term Frequency and Inverse Document Frequency(Tf-Idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b3209",
   "metadata": {},
   "source": [
    "term frequency:\n",
    "t-->word\n",
    "d-->document\n",
    "tf(t,d) = count of t in d / number of words in d\n",
    "\n",
    "document frequency:\n",
    "df(t) = occurrence of t in documents\n",
    "\n",
    "Inverse Document Frequency:\n",
    "df(t) = N(t)\n",
    "where\n",
    "df(t) = Document frequency of a term t\n",
    "N(t) = Number of documents containing the term t\n",
    "\n",
    "idf(t) = N/ df(t) = N/N(t)\n",
    "\n",
    "tf-idf(t, d) = tf(t, d) * idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9ed280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n",
      "for : 1.6931471805599454\n",
      "geeks : 1.2876820724517808\n",
      "r2j : 1.6931471805599454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heman\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# assign documents\n",
    "d0 = 'Geeks for geeks'\n",
    "d1 = 'Geeks'\n",
    "d2 = 'r2j'\n",
    "\n",
    "# merge documents into a single corpus\n",
    "string = [d0, d1, d2]\n",
    "\n",
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(string)\n",
    "\n",
    "# get idf values\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
    "    print(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4cbdbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'geeks': 1, 'for': 0, 'r2j': 2}\n",
      "\n",
      "tf-idf value:\n",
      "  (0, 0)\t0.5493512310263033\n",
      "  (0, 1)\t0.8355915419449176\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.54935123 0.83559154 0.        ]\n",
      " [0.         1.         0.        ]\n",
      " [0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    " \n",
    "# display tf-idf values\n",
    "print('\\ntf-idf value:')\n",
    "print(result)\n",
    " \n",
    "# in matrix form\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6adc27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
